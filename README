Machine Learning Model Comparison Project

Project Overview
Title: Comparative Analysis of Classification Models for Any Dataset.

Objective:

The goal of this project is to evaluate and compare the performance of multiple machine

learning classification models on a given dataset. The process involves splitting the dataset into
training and testing subsets, training at least five distinct models, optimizing their

hyperparameters, evaluating their performance using key metrics, and visualizing the results in
an interactive dashboard.

Problem Statement :

In many real-world classification tasks, selecting the best-performing model requires rigorous

comparison across multiple candidate models. This project aims to:

Divide the dataset into training and testing subsets.

Train at least five different classification models.

Record and compare their performances based on evaluation metrics such as accuracy,

precision, recall, F1-score, and optionally R² score (where applicable).

Perform hyperparameter tuning to optimize each model’s performance.

Build a dashboard to visualize and compare model performances.

2. Libraries used:

Library / Module Purpose Link
numpy (import
numpy as np)
Used for
numerical
operations,
efficient
handling of
arrays, and
mathematical
functions.
https://numpy.org/
pandas (import
pandas as pd)
For reading,
cleaning, and
manipulating
structured data
using
DataFrames
https://pandas.pydata.org/
train_test_split Splits the
dataset into
training and
testing subsets.

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.train_test_split.html
Grid Search CV Performs
hyperparameter
tuning using
cross-
validation to
find the best
combination of
model
parameters.

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.gridsearchCV.html
Standard scaler Scales features
to have zero
mean and unit
variance.
Useful for
models
sensitive to
feature
magnitude.

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.stanndardscaler.html
accuracy_score Computes the
accuracy of the
predictions.

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html
precision_score Measures the
proportion of
true positives
among all
predicted
positives.

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html
recall_score Measures the
proportion of
true positives
among all
actual
positives.

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html
f 1 _score Harmonic
mean of
precision and
recall —
balances the
two.

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html
r 2 _score Common in
regression but
occasionally
used for
performance
assessment in
classification.

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html
Classification_report Generates a
text report of
precision,
recall, F1-score
for each class.

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html
Logistic Regression A linear model
for binary
classification
problems.

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.logistic_regression.html
Decision Tree
Classifier

A tree-based
model that
splits data
based on
feature
thresholds.
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.decision_tree_classifier.html
Random Forest
Classifier

An ensemble
model that
builds multiple
decision trees
and averages
their results.
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.random_forest_classifier.html
SVC Support Vector
Classification
— a powerful
model for
linear/non-
linear
classification.

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.svc.html
XGB Classifier An efficient,
gradient-
boosted tree
classifier from
the XGBoost
library.
Excellent for
tabular data.
https://xgboost.readthedocs.io/
plotly.express
(import
plotly.express as px)
High-level
interactive
visualization
library for quick
plots like bar
charts, scatter
plots, etc.
https://plotly.com/python/plotly-
express/#:~:text=The%20plotly.express%20module%20(usually,for%20creating%20most%20common%20figures.
plotly.graph_objects
(import
plotly.graph_objects
as go)
Lower-level
Plotly interface
for creating
more
customized and
detailed
interactive
plots.
plotly.graph_objects (import plotly.graph_objects as go)
3. Methodology

3 .1 Data Preparation

Splitting:

Split the dataset into training (e.g., 80%) and testing (e.g., 20%) subsets using stratified

sampling to maintain class distribution.

Optionally, include a validation set or use cross-validation for hyperparameter tuning.

Preprocessing:

Handle missing values (e.g., imputation with mean/median or removal).

Encode categorical variables (e.g., one-hot encoding, label encoding).

Scale numerical features (e.g., Standard Scaler or Min Max Scaler).

Address class imbalance if applicable (e.g., oversampling with SMOTE, under sampling).

3 .2 Model Selection

Train and evaluate at least five classification models, such as:

Logistic Regression

Decision Tree

Random Forest

Support Vector Machine (SVM)

Gradient Boosting (e.g., XGBoost, LightGBM, or CatBoost )

2.3 Hyperparameter Tuning

For each model, optimize hyperparameters using techniques like:

Grid Search

Random Search

Bayesian Optimization (optional for efficiency)

Example hyperparameters:

Logistic Regression : C (regularization strength), solver

Random Forest : n_estimators, max_depth, min_samples_split

SVM : C, kernel type, gamma

Gradient Boosting : learning_rate, n_estimators, max_depth

Use cross-validation (e.g., 5-fold) to evaluate hyperparameter combinations.

2.4 Evaluation Metrics

Compute the following metrics for each model on the test set:

Accuracy : Proportion of correct predictions.

Precision : True positives divided by predicted positives.

Recall : True positives divided by actual positives.

F1-Score : Harmonic mean of precision and recall.

R² Score : (Optional, if applicable for specific classification contexts, e.g., ordinal
classification).

2.5 Visualization Dashboard

a. Visualizing Model Performance (Baseline vs Tuned)

We created a multi-faceted bar chart to compare the performance of each model across four
key metrics:

Accuracy

Precision

Recall

F1 Score

Each metric displays both the baseline and tuned version of the models side-by-side using
color-coded bars (blue for baseline, red for tuned).

This visualization helps identify how much performance improved after hyperparameter tuning.

Note: We ensured consistent model names across all metrics to avoid misalignment,
especially for SVM (Support Vector Machine).

b. Key Insights Summary

Best Overall Performer:

XGBoost (Tuned) consistently achieved the highest scores across most metrics, especially in
Precision and Accuracy, making it the s...

Key Insights Summary:

XGBoost (Tuned) had the highest overall performance across most metrics.
All models improved after tuning, especially in Precision and F1 Score.
SVM showed significant improvement in Recall.
Logistic Regression gave competitive results with low complexity.
Use Random Forest or XGBoost when high precision is require
